# -*- coding: utf-8 -*-
"""Random Forests

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/18IjwYxD-xBsMbqYA7rUmKCpa3G3mDrJy
"""

from google.colab import drive
drive.mount('/content/drive', force_remount = True)

from google.colab import drive
drive.mount('/content/drive')

"""
1. Import everything
"""

import librosa
import librosa.display
import matplotlib.pyplot as plt
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
from sklearn import datasets, linear_model
from sklearn.metrics import mean_squared_error, r2_score
from sklearn.metrics import confusion_matrix 
from sklearn.model_selection import train_test_split 
from sklearn.svm import SVC 
from sklearn.naive_bayes import GaussianNB
import math
from sklearn import preprocessing
from scipy import signal
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA

"""
2. Preprocessing function
-doesn't take any inputs
-internally calls csv's with sound file names and labels
-outputs verified, unverified, and the testing data
"""
def preprocessingfunc():

  #read in training and testing data as excel sheet
  excelsheet = pd.read_csv('/content/drive/My Drive/train (2).csv')
  exceltest = pd.read_csv('/content/drive/My Drive/sample_submission2.csv')

  #convert them to pandas dataframes
  df = pd.DataFrame(excelsheet)
  df2 = pd.DataFrame(exceltest)

  #create variable names for each of the columns
  fnamecolumn = df[['fname']]
  labelcolumn = df[['label']]
  verifiedcolumn = df[['manually_verified']]

  fnamecolumntest = df2[['fname']]

  #sort the dataframe so the verified columns are at the top and count them
  #sort_verified.sum(axis = 0, skipna = True) ends up being 1256
  sort_verified = excelsheet.sort_values(by='manually_verified', ascending=False)
  trainingsize = len(sort_verified.index)

  #split the data set
  verified = pd.DataFrame(sort_verified[0:1256])
  unverified = pd.DataFrame(sort_verified[1256:trainingsize])

  return verified,unverified,fnamecolumntest

"""
3. Lowpass and Feature Extract for TRAINING Data
-input names of raw sound files
-for each, low pass filter and compute it's spectrogram
-outputs a dataframe with the labels and the spectrograms
"""

def lowpassandfeatures(data):

  #assign variables to columns of the input dataframe
  fnamecolumn = data[['fname']]
  labelcolumn = data[['label']]

  #create empty lists
  label = []
  spectrums = []

  #iterate through each audio signal
  for row in fnamecolumn.iterrows():
    
    #load the audio file
    stringrow = str(row[1])
    y, fs = librosa.load("/content/drive/My Drive/301-project.audio_train/"+stringrow[9:21],44100)
    
    #lowpass filter the noise out
    fc = 30  # Cut-off frequency of the filter
    w = fc / (fs / 2) # Normalize the frequency

    b, a = signal.butter(5, 0.82, 'low')
    output = signal.filtfilt(b, a, y)

    #compute the melspectrum and add it to a list
    melspect = librosa.feature.melspectrogram(output, sr=fs)[0]
    spectrums.append(melspect)

    #provide a visual progress counter
    print('count1 = ', len(spectrums))

  #iterate through the labels and add them to a list
  for row in labelcolumn.itertuples():
    label.append(row[1])
  
  #create a new dataframe with the labels and the spectrums
  data_tuples = list(zip(label,spectrums))
  newdata = pd.DataFrame(data_tuples)

  return(newdata)

"""
4. Lowpass and Feature Extract for TESTING Data
-input names of raw sound files
-for each, low pass filter and compute it's spectrogram
-outputs a dataframe with the spectrograms
"""

def lowpassandfeaturestest(data):

  #assign variables to columns of the input dataframe
  fnamecolumn = data[['fname']]

  #create empty lists
  label = []
  dimensions = []

  #iterate through each audio file
  for row in fnamecolumn.iterrows():

    #load the audio file
    stringrow = str(row[1])
    y, fs = librosa.load("/content/drive/My Drive/301-project.audio_test/"+stringrow[9:21],44100)
    
    #lowpass filter the noise out
    fc = 30  # Cut-off frequency of the filter
    w = fc / (fs / 2) # Normalize the frequency
    b, a = signal.butter(5, 0.82, 'low')
    output = signal.filtfilt(b, a, y)

    #compute the melspectrum and add it to the list
    melspect = librosa.feature.melspectrogram(output, sr=fs)[0]
    dimensions.append(melspect)

    #add a zero to another list
    #(note that this is a useless column, but makes sure that the dataframe is in the right format later)
    label.append(0)

    #provide a visual progress counter
    print('count1 = ', len(dimensions))

  #recombine everything into a dataframe, but only output the column of spectrograms  
  data_tuples = list(zip(dimensions,label))
  newdata = pd.DataFrame(data_tuples)
  return(newdata[0])

'''
5. Zeropad function
-input dataframe of spectrograms
-zeropads each to the maximum length
-outputs a list with the zeropadded spectrograms
'''
def zeropad(data,length):

  #turn the data into an array
  data = np.array(data)

  #set a variable for the maxlength to zero, and create an empty list
  maxlength = 0
  newdata =[]

  #compare the length of each spectrum to maxlength, the maximum of the previous checked data
  for i in range(len(data)):
    newlength = len(data[i])

    #make maxlength the new length if it's greater
    if (maxlength < newlength):
      maxlength = newlength

  #if the inputted length is higher, make maxlength equal to the input length
  if (length > maxlength):
    maxlength = length

  #zeropad each spectrogram out to the maximum length and add it to a list
  for j in range(len(data)):
    num = (maxlength-len(data[j]))/2
    newdata.append (np.pad(data[j], (math.ceil(num),math.floor(num)),'mean',))
  return newdata, maxlength

"""
5. Model to also take testing data:
-input data to train model
-output predictions for labels
"""
'''
def model(x_train, y_train, x_test):

  #import new libraries
  from sklearn.neighbors import KNeighborsClassifier

  #create a model and fit it to the data
  neigh = KNeighborsClassifier(n_neighbors=9)
  neigh.fit(x_train, y_train)

  #use it to predict labels for the testing data
  predictions = neigh.predict(x_test)

  return predictions
'''
def model(x_train,y_train,x_test):

  #Import new libraries
  from sklearn.metrics import confusion_matrix 
  from sklearn.model_selection import train_test_split 
  from sklearn.svm import SVC 
  
  clf = RandomForestClassifier(random_state=0)
  clf.fit(x_train, y_train)

  randomforest_predictions = clf.predict(x_test)

  return randomforest_predictions

"""
6. Model to complete intial tests:
-take training data and separate it into testing and training
-output a confusion matrix to measure accuracy of the model
"""
'''
def modelwithtest(x_train,y_train):
#KNN
  #import new libraries
  from sklearn.metrics import confusion_matrix 
  from sklearn.model_selection import train_test_split 
  from sklearn.svm import SVC 
  from sklearn.neighbors import KNeighborsClassifier

  #create the model
  neigh = KNeighborsClassifier(n_neighbors=5)
  
  #split data into training and testing
  x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.33)

  #train the model and use it to predict labels
  neigh.fit(x_train, y_train)
  predictions = neigh.predict(x_test)

  #create a confusion matrix
  cm = confusion_matrix(y_test, predictions)

  return predictions, cm, x_test
'''
def modelwithtest(x_train,y_train):

  #Import new libraries
  from sklearn.metrics import confusion_matrix 
  from sklearn.model_selection import train_test_split 
  from sklearn.svm import SVC 
  from sklearn.neighbors import KNeighborsClassifier
  
  print(696)
  #Split data
  x_train, x_test, y_train, y_test = train_test_split(x_train, y_train, test_size = 0.33)

  #print(697)
  ##scale
  #scaler = StandardScaler()

  #print(698)
  ## Fit on training set only.
  #scaler.fit(x_train)

  #print(699)
  ## Apply transform to both the training set and the test set.
  #x_train = scaler.transform(x_train)
  #x_test = scaler.transform(x_test)
  #print(700)



  ## Make an instance of the PCA Model
  #pca = PCA(.95)  
  #print(701)
  ## Fit PCA modelon training set only
  #pca.fit(x_train)
  #print(702)
  ## Apply PCA model on train and test
  #x_train = pca.transform(x_train)
  #x_test = pca.transform(x_test)
  #print(703)


  # creat and apply random forest
  clf = RandomForestClassifier(random_state=0, criterion = 'entropy',min_samples_split = 6)
  print(704)
  clf.fit(x_train, y_train)
  print(705)
  randomforest_predictions = clf.predict(x_test)

  cm = confusion_matrix(y_test, randomforest_predictions)

  return randomforest_predictions, cm

"""
HERE'S WHERE WE START OUR PROCESS AND CALL THE ABOVE FUNCTIONS
"""

#Step One: Call preprocessing (box 2) to pull in the data and separate it
verified = preprocessingfunc()[0]
#unverified = preprocessingfunc()[1]
print(1)

#Step Two: Lowpass and Extract Features from verified and unverified
verifiedfeatures = lowpassandfeatures(verified[0:300])
#unverifiedfeatures = lowpassandfeatures(unverified[0:50])
print(2)

#Step Three: Convert Labels to Numbers
#     (This was in a separate function, but was removed to make it easier to convert them back)
#     We fit an encoder to the verified data, and then use that to change the labels into numbers
#     The same encoder is used for verified and unverified for consistency
le = preprocessing.LabelEncoder()
le.fit(verifiedfeatures[0])
verifiedfeatureslabels = verifiedfeatures.copy()
verifiedfeatureslabels[0] = le.transform(verifiedfeatureslabels[0])
#unverifiedfeatureslabels = unverifiedfeatures.copy()
#unverifiedfeatureslabels[0] = le.transform(unverifiedfeatureslabels[0])
#saved = verifiedfeatureslabels.to_csv("/content/drive/My Drive/Colab Notebooks/verifiedfeatureslabels.csv")
#saved = unverifiedfeatureslabels.to_csv("/content/drive/My Drive/Colab Notebooks/unverifiedfeatureslabels.csv")
print(3)


#Step Four: Convert the data to a usable form
#     separate the columns and make them arrays
#     zeropad the data to the same length

x_trainlabels = np.array(verifiedfeatureslabels[0])
x_traindata = np.array(verifiedfeatureslabels[1])
x_trainpadded = zeropad(x_traindata,0)[0]
maxlength = zeropad(x_traindata,0)[1]


#Step Five: Call Model on verified training data and print the confusion matrix

verifiedoutput = modelwithtest(x_trainpadded,x_trainlabels)
cm = verifiedoutput[1]
print ("confusion matrix",cm)
print ("summ",cm.sum())
print ('percentage', cm.trace()/cm.sum())
print(5)


'''
#Step Six: Train the model on verified and use it to test unverified
unverdata = np.array(unverifiedfeatureslabels[1])
unverpadded = zeropad(unverdata, maxlength)[0]
newmax = zeropad(unverdata, maxlength)[1]
if (newmax > maxlength):
  x_trainpadded = zeropad(x_traindata,newmax)[0]
unverifiedpredictions = model(x_trainpadded, x_trainlabels, unverpadded)
print(6)

#Step Seven: Convert the unverified predictions to labels
unverifiednames = list(le.inverse_transform(unverifiedpredictions))
unverifiedmodeled = unverifiedfeatureslabels.copy()
unverifiedmodeled['labels'] = unverifiedpredictions
 
print(7)

#Step Eight: Compare the predictions to the unverified labels

print(unverifiedmodeled)
unverified = unverified.reset_index(drop=True)
print(unverified)
unverifiedmodeled.rename({0: 'label1'}, axis=1, inplace=True)
for index, row in unverifiedmodeled.iterrows():
  if (row['labels'] != row['label1']):
    print('false',index)
    unverified.drop([index], axis = 0, inplace = True)
print('newunverified', unverified)

print(verified)
verified= verified.append(unverified, ignore_index = True)
saved = verified.to_csv("/content/drive/My Drive/Colab Notebooks/verifiedandcheckedunverified2.csv")
print(verified)
print(8)



#Step Nine: Prepare data to use in testing data
x_trainlabels = np.array(verifiedfeatureslabels[0])
x_traindata = np.array(verifiedfeatureslabels[1])
x_trainpadded = zeropad(x_traindata,0)[0]
maxlength = zeropad(x_traindata,0)[1]
print(9)

#Step Ten(ish): Preprocess the testing data
x_test = preprocessingfunc()[2]
x_testfeatures = lowpassandfeaturestest(x_test)
#saved = x_testfeatures.to_csv("/content/drive/My Drive/Colab Notebooks/x_testfeatures.csv")
testdata = np.array(x_testfeatures)
print(10)

#Step Eleven: Zeropad it to the maximum length of the training data
#     output the maximum and check it against the maximum of the training data
#     if the testing had a larger maximum, rezeropad the training data to the longer length
testpadded = zeropad(testdata,maxlength)[0]
newmax = zeropad(testdata, maxlength)[1]
if (newmax > maxlength):
  x_trainpadded = zeropad(x_traindata,newmax)[0]
print(11)

#Step Twelve: Run the model on the testing data
predictions = model(x_trainpadded, x_trainlabels, testpadded)
print(12)

#Step Thirteen: Retransform the labels and reformat the output
predictionnames = list(le.inverse_transform(predictions))
x_test['labels'] = pd.Series(predictionnames, index=x_test.index)
print(x_test)
#saved = x_test.to_csv("/content/drive/My Drive/Colab Notebooks/knnnewtry.csv")
print(13)
'''